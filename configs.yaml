---
- name: good_GW_GPT
  condition: Ours
  n_layer: 10
  n_head: 4
  n_embd: 128
  dropout: 0.22
  learning_rate: 0.0005
  gradient_accumulation_steps: 32
  max_router_iter: 10
  max_iters: 61700
  model_cls: GW_GPT
  batch_size: 2 # micro batch size if gradient_accumulation_steps > 1
# - name: equivalent_transformer
#   condition: Transformer
#   n_layer: 10
#   n_head: 4
#   n_embd: 128
#   dropout: 0.22
#   learning_rate: 0.0005
#   gradient_accumulation_steps: 32
#   max_iters: 61700
#   # lr_decay_iters: 10000
#   model_cls: GPT
#   batch_size: 2 # micro batch size if gradient_accumulation_steps > 1
# - name: fast_GW_GPT # for quick debugging
#   condition: Ours
#   n_layer: 1
#   n_head: 1
#   n_embd: 32
#   dropout: 0.44293320242330486
#   learning_rate: 1.652170920104858e-05
#   max_router_iter: 2
#   max_iters: 30
#   batch_size: 2
#   eval_iters: 3
#   eval_interval: 10
#   model_cls: GW_GPT
# - name: bad-transformer # val_loss: 6.33
#   condition: Transformer
#   n_layer: 3
#   n_head: 4
#   n_embd: 96
#   dropout: 0.44293320242330486
#   learning_rate: 1.652170920104858e-05
#   max_iters: 1900
#   model_cls: GPT
# - name: good-transformer # val_loss: 2.99
#   condition: Transformer
#   n_layer: 5
#   n_head: 16
#   n_embd: 960
#   dropout: 0.2214670000693801
#   learning_rate: 0.0005539426893794069
#   max_iters: 61700
#   model_cls: GPT
